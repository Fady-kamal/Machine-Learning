{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-armed Bandit Problem\n",
    "\n",
    "In this lab, we will apply a complete bandit algorithm using  <b> $ \\large \\epsilon$-greedy algorithm</b> on K-armed bandit problem.\n",
    "$ \\large \\epsilon$-greedy algorithm is an algorithm used to make tradeoff between exploration and exploitation techniques so we can get the optimal action. it represents an exploration strategy where the agent behave greedily most of the time but every once in a while , the agent explore all the available actions with probability $ \\large \\epsilon $\n",
    "\n",
    "     \n",
    "     \n",
    "<b>In the K-armed Bandit problem: </b>\n",
    "- The agent is faced repeatedly  with a choice among $K$ differenet choices  or actions \n",
    "- After each action , the agent receive a reward chosen from a stationary probability distribution which depends on the action the agent selected \n",
    "- The agent objective is to maximize the expected total reward over some time period \n",
    "\n",
    "<b> Why do we call it \"K-armed Bandit\" ? </b>\n",
    "\n",
    "It is named by analogy to a slot machine or \"one-armed bandit machine\" expect that it takes K levers instead of one \n",
    "\n",
    "The slot machine is designed as follows: \n",
    "\n",
    "<img src  = \"https://secure.img1-fg.wfcdn.com/im/00280517/resize-h800-w800%5Ecompr-r85/1548/15481915/Vegas+Slot+Machine+Cardboard+Standup.jpg\" width = \"50%\"> \n",
    " \n",
    "- Each time we pull the lever, we get a reward taken from a stationary probability distribution \n",
    "- Our goal is to find out which slot machine will give us the maximum cumulative rewards over a sequence of time\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "The pseudocode for a complete bandit algorithm using incrementally computed sample\n",
    "averages and  $ \\large \\epsilon$-greedy algorithm is shown in the box below. \n",
    "<img src = \"https://i.imgur.com/vdPAifG.png\" > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import gym_bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-10-31 16:10:03,662] Making new env: BanditTenArmedGaussian-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BanditTenArmedGaussian-v0\") # Replace with relevant env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore our action space \n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-greedy Algorithm \n",
    "\n",
    "Lets create a function for espilon greedy algorithm part. The pseudocode of the algorithm is shown in the box below \n",
    "\n",
    "<img src = \"https://i.imgur.com/Rsh8mrf.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon, Q):\n",
    "    '''\n",
    "    Usage:\n",
    "      #epsilon_greedy --> used for construct epsilon greedy algorithm used in the bandit algorithm \n",
    "                                          \n",
    "    Arguments:\n",
    "      #epsilon --> the probabilty of taking an exploratory action \n",
    "      #Q --> a numpy array hold the value of every action in the action space \n",
    "    \n",
    "    Returns:\n",
    "      #action --> the selected action at a certain time \n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #get a random number from  a continuous uniform distribution over the interval [0.0, 1.0)\n",
    "    rand = np.random.random()\n",
    "    \n",
    "    #if the random value is less than epsilon \n",
    "    if rand < epsilon: \n",
    "        \n",
    "        action = env.action_space.sample() #take a random action from the action space \n",
    "        \n",
    "    else: \n",
    "        \n",
    "        action = np.argmax(Q) #take a greedy action \n",
    "        \n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit(epsilon):\n",
    "    '''\n",
    "    Usage:\n",
    "      #bandit --> used to get the optimal action should be selected \n",
    "                                          \n",
    "    Arguments:\n",
    "      #epsilon --> the probabilty of taking an exploratory action \n",
    "    \n",
    "    Returns:\n",
    "      #The optimal action\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #initialize the number of iterations \n",
    "    num_iters = 20000\n",
    "    \n",
    "    #pre-allocating an empty array which will hold the number of times every action is taken \n",
    "    count = np.zeros(10)\n",
    "    \n",
    "    #pre-allocating an empty array which will hold the sum of rewards corresponding to every action when it is taken\n",
    "    sum_rewards = np.zeros(10)\n",
    "    \n",
    "    #pre-allocating an empty array which will hold the estimated action value for every action \n",
    "    Q = np.zeros(10)\n",
    "    \n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # make an action selection\n",
    "        action = epsilon_greedy(epsilon, Q) \n",
    "        \n",
    "        # get the reward corresponding to this action\n",
    "        observation, reward, done , info = env.step(action)\n",
    "        \n",
    "        # increment the count corresponding to this action \n",
    "        count[action] += 1\n",
    "        \n",
    "        # acummualte the rewards obtained from this action \n",
    "        sum_rewards[action] += reward \n",
    "        \n",
    "        # estimate the value of this action \n",
    "        Q[action] = sum_rewards[action] / count[action]\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"The optimal action is {np.argmax(Q)}\")\n",
    "    \n",
    "    return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal action is 7\n"
     ]
    }
   ],
   "source": [
    "optimal_action = bandit(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
