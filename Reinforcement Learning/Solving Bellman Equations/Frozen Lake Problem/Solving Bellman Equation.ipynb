{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving The Frozen Lake Problem\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## The Problem Details\n",
    "\n",
    "* Imagine there is a frozen lake stretching from your home to your office; you have to walk\n",
    "on the frozen lake to reach your office.\n",
    "* But oops! There are holes in the frozen lake so you have to be careful while walking on the frozen lake to avoid getting trapped in the holes.\n",
    "\n",
    "\n",
    "<img  src = \"https://i.imgur.com/AC8YjF3.png\" style=\"width:300px;height:300px;\">\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "<strong>The symbols in the above picture:</strong> \n",
    "\n",
    " * <strong>S:</strong> represents the strating position <strong>(home)</strong>\n",
    " * <strong>F:</strong> represents the frozen lake where you can walk\n",
    " * <strong>H:</strong> represents the holes, which you have to be so careful\n",
    " * <strong>G:</strong> reprensents the goal <strong>(office)</strong>\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<strong>The agent's goal :</strong> is to find the optimal path to go from  <strong>S</strong> to <strong>G</strong> without getting trapped at  <strong>H</strong>.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "The  <strong>MDP</strong> we have consists of the following:\n",
    " * <strong>States :</strong> Set of states. Here we have 16 states (each little square box in the grid).\n",
    " * <strong>Actions :</strong> Set of all possible actions (left, right, up, down; these are all the four possible actions our agent can take in our frozen lake environment).\n",
    " * <strong>Transition Function $P^{a}_{s,s^{\\prime}}$:</strong> The probability of moving from one state <strong>(F)</strong> to another state <strong>(H)</strong> by performing an action a.\n",
    " * <strong>Reward Function  $R^{a}_{s,s^{\\prime}}$:</strong> This is the  expected reward we can recieve while moving from one state <strong>(F)</strong> to another state <strong>(H)</strong> by performing an action a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<strong>The two possible ways to solve MDP in order to get the optimal policy:</strong>\n",
    "\n",
    " * [Value Iteration](#VI)\n",
    " * [Policy Iteration](#PI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "\n",
    "import gym \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-08-19 17:33:57,743] Making new env: FrozenLake-v0\n",
      "C:\\Users\\FADY-PC\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:18: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "#Make our frozen lake enviroment using OpenAI's Gym\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of states: 16\n"
     ]
    }
   ],
   "source": [
    "#Explore the number of states in the enivronment \n",
    "print(f'The number of states: {env.observation_space.n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "#Explore the number of actions in the environment \n",
    "print(f'The number of actions: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration <a anchor = \"anchor\" id = \"VI\" />\n",
    "\n",
    "## Recall the state-action value function: \n",
    "\n",
    "\\begin{equation}\n",
    "\\large Q(s,a) = \\sum_{s^{\\prime}} {[R^{a}_{s,s^{\\prime}} + V(s^{\\prime})] \\hspace{1mm}P^{a}_{s,s^{\\prime}}} = \\sum_{s^{\\prime}} { next\\hspace{1mm}state\\hspace{1mm}reward\\hspace{1mm} for\\hspace{1mm}every\\hspace{1mm}state\\hspace{1mm}(s^{\\prime})}\n",
    "\\end{equation}\n",
    "\n",
    "## The Pseudo-code \n",
    "\n",
    "<img src = \"https://i.imgur.com/vOAXnT0.png\">\n",
    "\n",
    "\n",
    "## The steps involved in the value iteration are as follows:\n",
    "1. First, we initialize the random value function, that is, the random value for each\n",
    "state.\n",
    "2. Then we compute the Q function for all state action pairs of Q(s, a).\n",
    "3. Then we update our value function with the max value from Q(s,a).\n",
    "4. We repeat these steps until the change in the value function is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "      #value_iteration--> used for solving MDP by finding the optimal value function for all states\n",
    "  \n",
    "    Arguments:\n",
    "      #env --> The enviroment that the agent interacts with\n",
    "      #gamma --> represents the discount factor. The defalut value is 1.0\n",
    "    \n",
    "    Returns:\n",
    "      #value_table -->is a the table of  the optimal value functions for all the states\n",
    "      #Q_value --> list of the updated values of the state-action value function for all\n",
    "                   states\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #First, we initialize the random value table which is 0 for all the states \n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #Define the number of iterations\n",
    "    no_of_iterations = 10000\n",
    "    \n",
    "    #Keep until convergence\n",
    "    for i in range(no_of_iterations):\n",
    "        #Upon starting each iteration, we copy the value_table to update_value_table\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        #For every space in the environment\n",
    "        for state in range(env.observation_space.n):\n",
    "            \n",
    "            #instead of creating a Q-table for each state, we create a list listing \n",
    "            #all the values of state-action pair \n",
    "            Q_value = []\n",
    "            \n",
    "            #Scan every action \n",
    "            for action in range(env.action_space.n):\n",
    "                \n",
    "                #Define empty list for storing next state reward for every transition state\n",
    "                next_states_rewards = []\n",
    "\n",
    "                #Get some useful values of every state-action pair\n",
    "                for next_sr in env.P[state][action]:\n",
    "                    \n",
    "                    trans_prob, next_state, reward_func, _ = next_sr\n",
    "                    #append the next-state reward value for that state-action pair\n",
    "                    next_states_rewards.append((trans_prob * (reward_func + gamma * updated_value_table[next_state])))\n",
    "\n",
    "                #append the some of next_states_rewards for all the successor states for\n",
    "                #every state-action pair \n",
    "                Q_value.append(np.sum(next_states_rewards))\n",
    "                \n",
    "            #Pick up the maximum Q value and update it as value of a state\n",
    "            value_table[state] = max(Q_value)\n",
    "            \n",
    "        \n",
    "        #check if we have reached the convergence, that is, the difference of \n",
    "        #the value-function between each iteration is small\n",
    "        #at a result of that, we define a threshold ,a cutoff value, which we stop updating if \n",
    "        #the difference at least equals it \n",
    "        threshold = 1e-10  \n",
    "        \n",
    "        if(np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "            print(f\"Value-iteration converged at iteration: {i+1}\")\n",
    "            break\n",
    "            \n",
    "    return value_table, Q_value        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration: 877\n"
     ]
    }
   ],
   "source": [
    "#We can get the optimal value-function for all states(value_table) and the corresponding  Q_values \n",
    "value_table, Q_value  = value_iteration(env = env, gamma= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n"
     ]
    }
   ],
   "source": [
    "#Print the values of the optimal value function \n",
    "print(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#Print the Q_values\n",
    "print(Q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "<strong>After finding the optimal value function, how can we extract the optimal policy from the\n",
    "optimal_value_function?</strong> \n",
    "* We calculate the Q value using our optimal value action and pick up the actions which have the highest Q value for each state as the optimal policy. \n",
    "* We do this via a function called <strong>extract_policy()</strong>\n",
    "\n",
    "\n",
    "## Note:\n",
    "\n",
    "The functions, value_iteration() and extrac_policy(), <strong>together</strong> represent <strong>the value iteration algorithm</strong>, but in the lab we divide the algorithm into two functions just for explanation but we can encapsulate the two functions into one and call it value_iteraion()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table,gamma = 1.0):\n",
    "    '''\n",
    "    Usage:\n",
    "      #extract_policy--> used for getting the optimal policy\n",
    "  \n",
    "    Arguments:\n",
    "      #value_table -->is a the table of  the optimal value functions for all the states\n",
    "      #gamma --> represents the discount factor. The defalut value is 1.0\n",
    "    \n",
    "    Returns:\n",
    "      #policy--> list represents the best actions to perform for each state we the agent starts from\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Define a random policy for all the states\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #for each state , we build Q_table holds the possible action we can perform from this state\n",
    "    for state in range(env.observation_space.n):\n",
    "        \n",
    "        #pre-allocating the Q_table\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        \n",
    "        #for each action in the state, we compute the its state-action value function, Q, \n",
    "        #And, append it to Q_table\n",
    "        for action in range(env.action_space.n):\n",
    "            #Get some useful values of every state-action pair\n",
    "                for next_sr in env.P[state][action]:\n",
    "                    \n",
    "                    trans_prob, next_state, reward_func, _ = next_sr\n",
    "                    \n",
    "                    #Compute state-action value function for a certain (state,action) pair\n",
    "                    #And add it to Q_table\n",
    "                    Q_table[action] += trans_prob * (reward_func + gamma * value_table[next_state])\n",
    "                    \n",
    "                    \n",
    "        #Then Pick the action that has the highest Q_value for a specific state\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "        \n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the optimal policy for each state\n",
    "optimal_policy = extract_policy(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the optimal_policy\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Policy Iteration <a anchor = \"anchor\" id = \"PI\" />\n",
    "\n",
    "<br>\n",
    "    \n",
    "## The Pseudo-code \n",
    "\n",
    "<img src = \"https://i.imgur.com/rOwrfIO.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## The steps involved in the policy iteration are as follows: :\n",
    "\n",
    "1. First, we initialize some random policy\n",
    "2. Then we find the value function for that random policy and evaluate to check if it is optimal which is called policy evaluation\n",
    "3. If it is not optimal, we find a new improved policy, which is called policy improvement\n",
    "4. We repeat these steps until we find an optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(policy,gamma = 1.0):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "      #compute_value_function--> Compute value function given a policy\n",
    "  \n",
    "    Arguments:\n",
    "      #policy --> the policy that we compute the value fuction based on it\n",
    "      #gamma --> represents the discount factor. The defalut value is 1.0\n",
    "      \n",
    "    Returns:\n",
    "    \n",
    "      #value_table--> is a the table of  the updated value functions for all the states which yield \n",
    "                      to a better policy using them\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #first, we initialize the value_table as zeros for all the states\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #check if we have reached the convergence, that is, the difference of \n",
    "    #the value-function between each iteration is small\n",
    "    #at a result of that, we define a threshold ,a cutoff value, which we stop updating if \n",
    "    #the difference at least equals it \n",
    "    threshold = 1e-20  \n",
    "    \n",
    "    \n",
    "    #Keep going until convergence\n",
    "    while(True):\n",
    "        \n",
    "        #for each state, we get an action to perfom under the random policy we intialize\n",
    "        #we compute the value function according to that action and the state\n",
    "        #we make a new variable called updated_value_table to update the value table using it \n",
    "        updated_value_table = np.copy(value_table)\n",
    "\n",
    "        #for every state in the environment\n",
    "        for state in range(env.observation_space.n):\n",
    "\n",
    "            #get the action the agent must perform under that policy \n",
    "            action = policy[state]\n",
    "            \n",
    "            #compute the value_function under the given policy\n",
    "            value_table[state] = sum([trans_prob * (reward_func + gamma * updated_value_table[next_state]) for trans_prob, next_state, reward_func, _ in env.P[state][action]])\n",
    "\n",
    "            \n",
    "\n",
    "        #Check for cenvergence\n",
    "        if(np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "            break\n",
    "\n",
    "            \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,gamma = 1.0):\n",
    "    '''\n",
    "    Usage:\n",
    "      #policy_iteration--> used for solving MDP by finding the optimal policy the agent must follow\n",
    "                           \n",
    "  \n",
    "    Arguments:\n",
    "      #env --> The enviroment that the agent interacts with\n",
    "      #gamma --> represents the discount factor. The defalut value is 1.0\n",
    "    \n",
    "    Returns:\n",
    "      #new_policy --> new_policy --> represents array of the optimal policy for each state\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #first, we initialize a random policy\n",
    "    random_policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #define the number of iterations\n",
    "    no_of_iterations = 20000\n",
    "    \n",
    "    #then, for each iteration we calculate the new value function corresponding to the random policy\n",
    "    #Extract the optimal policy for each state using the new value function we calculate\n",
    "    for i in range(no_of_iterations):\n",
    "        new_value_function = compute_value_function(random_policy,gamma = 1.0)\n",
    "        new_policy = extract_policy(new_value_function)\n",
    "        \n",
    "        #then, we check whether we have reached convergence\n",
    "        #we do that by comparing the random policy with the new policy\n",
    "        #if the random policy equals the policy we break \n",
    "        #else, we update the random_policy with the new policy we get from new value function \n",
    "        if (np.all(random_policy == new_policy)):\n",
    "            print(f\"Policy Iteration converged at iteration: {i+1}\")\n",
    "            break\n",
    "            \n",
    "        random_policy = new_policy\n",
    "        \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration converged at iteration: 7\n"
     ]
    }
   ],
   "source": [
    "#Get the optimal Policy for each state\n",
    "optimal_policy = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the optimal policy\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
