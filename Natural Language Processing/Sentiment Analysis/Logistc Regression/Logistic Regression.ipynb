{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis \n",
    "In this lab, we will build a sentiment analysis system which detects the attitude of the text . we build the system based on the logistic regression model.\n",
    "\n",
    "\n",
    "# Outline \n",
    "* [Import Functions, Libraries, and Data](#1)\n",
    "* [Helper Functions](#2)\n",
    "* [Pre-process the data](#3)\n",
    "* [Logistic Regression](#4)\n",
    " * [Sigmoid Function](#4.1)\n",
    " * [Compute the Cost Function](#4.2)\n",
    " * [Compute the Gradient](#4.3)\n",
    "* [Extracting the Features](#5)\n",
    " * [Feature Extraction for a Single Tweet](#5.1)\n",
    " * [Feature Extraction for all the Tweets in the Training Set](#5.2)\n",
    "* [Training the Model](#6)\n",
    "* [Test your logistic regression](#7)\n",
    "* [Check Performance Using the Test Set](#8)\n",
    "* [Error Analysis](#9)\n",
    "* [Predict with your Own Tweet](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Functions, Libraries, and Data <a anchor = \"anchor\" id = \"1\" > </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries and functions\n",
    "import numpy as np \n",
    "import math \n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlaod the data needed for that lab\n",
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data \n",
    "from nltk.corpus import twitter_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions <a anchor = \"anchor\" id = \"2\"></a>\n",
    "\n",
    "We will implement some  helper functions that help us build the System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Usage:\n",
    "      #process_tweet --> used to clean the text, tokenize it into separate words, remove stopwords, \n",
    "                         and convert words to stems.\n",
    "      \n",
    "    Arguments:\n",
    "      #tweet --> a string containing  a tweet \n",
    "    \n",
    "    Returns:\n",
    "      #tweet_clean --> a list of words containing  the processed tweet\n",
    "    '''\n",
    "    \n",
    "    #create a new Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    #get the latest of all the English stop words \n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #define an empty list which will hold the cleaned tokens\n",
    "    tweets_clean = []\n",
    "    \n",
    "    #loop over every tokens in the list of tokens, tweets_tokens\n",
    "    for token in tweet_tokens:\n",
    "        if (token not in stopwords_english and token not in string.punctuation):\n",
    "            \n",
    "            #stemming that token \n",
    "            stem_token = stemmer.stem(token)\n",
    "            \n",
    "            #append the stem of the token in the tweets_clean list\n",
    "            tweets_clean.append(stem_token)\n",
    "            \n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    '''\n",
    "    Usage:\n",
    "      #bulid_freqs --> used to count how often a word in the 'corpus' (the entire set of tweets) was \n",
    "                       associated with a positive label '1' or a negative label '0', then builds \n",
    "                       the freqs dictionary, where each key is a (word,label) tuple, \n",
    "                       and the value is the count of its frequency within the corpus of tweets.\n",
    "      \n",
    "    Arguments:\n",
    "      #tweets --> a list of tweets \n",
    "      #ys --> a m x 1 array holds the sentiment label or the class (0 or 1) corresponds to every tweet\n",
    "    \n",
    "    Returns:\n",
    "      #freqs--> a dictionary whose key is (word, sentiment label (class)) and whose value frequency\n",
    "                     of the word which is the number of times that word showes up in that class\n",
    "    '''\n",
    "    \n",
    "    #reduce the rank of the ys array to be rank one array, then, convert it to list \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    \n",
    "    #initialize freqs dic as empty dic which will be populated by looping over every tweet in tweets\n",
    "    freqs = {}\n",
    "    \n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            \n",
    "            #if the key exist in the dic, increment the value by one \n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            #if not, intialize its value to one \n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data <a anchor = \"anchor\" id = \"3\"></a>\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
    "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the set of positive and negative tweets \n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test set \n",
    "train_pos = all_positive_tweets[ :4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating the training tweets, positive and negative \n",
    "train_x  = train_pos + train_neg \n",
    "\n",
    "#conatenating the testing tweets, positive and negatives\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the training tweets \n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the test tweets\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the negative and postive labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore train_y \n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore test_y \n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create frequency dictionary \n",
    "freqs = build_freqs(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the freq dictionary \n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the length of the frequency dic \n",
    "len(freqs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the process_tweet function\n",
    "\n",
    "The given function `process_tweet()` tokenizes the tweet into individual words, removes stop words and applies stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Logistic Regression  <a anchor = \"anchor\" id = \"4\"></a>\n",
    "\n",
    "### Sigmoid Function <a anchor = \"anchor\" id = \"4.1\"></a>\n",
    "The function is named sigmoid as their graph looks like an $S$.\n",
    "\n",
    "To create probability, we'll pass $z$ through the sigmoid function , $ h(z)$.\n",
    "\n",
    "The sigmoid has the following equation , which shown graphically in the below Figure:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+e^{-z}} \\tag{1} = \\frac{1}{1 + e^{-\\theta^{T}X}}$$\n",
    "\n",
    "<img  src = \"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width = \"50%\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X,theta):\n",
    "    '''\n",
    "    Usage:\n",
    "      #sigmoid --> Computes sigmoid of z = X𝜃 element-wise.\n",
    "\n",
    "    Arguments:\n",
    "      #X --> The Design Matrix\n",
    "      #theta --> The Parameters which need to update\n",
    "\n",
    "    Returns:\n",
    "      #sigmoid ---> The computed vlaue of sigmoid(z)\n",
    "    '''\n",
    "    #Compute  Our linear-hypothesis function \n",
    "    z = np.matmul(X,theta)\n",
    "\n",
    "    #computes the sigmoid of z\n",
    "    sigmoid = np.divide(1, (np.add(1, np.exp(-z))))\n",
    "        \n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function  <a anchor = \"anchor\" id = \"4.2\"></a>\n",
    "\\begin{equation}\n",
    "  CE = \\sum_{i=1}^{m} {Loss(y_{pred},y)} =\\frac{1}{m}\\sum_{i=1}^{m} {(y^{(i)})(-\\log(y_{pred}^{(i)})) - (l-y^{(i)})(\\log(1-y_{pred}^{(i)}))}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta):\n",
    "    '''\n",
    "    Usage:\n",
    "      #computCost --> computes the cost for logistic regression\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #X --> The Design Matrix\n",
    "      #y --> The Ground Truth\n",
    "      #theta --> The Parameters which need to update\n",
    "    \n",
    "    Returns:\n",
    "      #J --> The cost value\n",
    "    '''\n",
    "    #Compute m --> the number of training featue vectors\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Compute  Our non-linear hypothesis function \n",
    "    h = sigmoid(X,theta)\n",
    "    \n",
    "    #Compute the losses\n",
    "    losses = np.subtract(np.multiply(-y,np.log(h)), np.multiply((1-y),np.log(1-h)))\n",
    "     \n",
    "    #Compute the Cross Entropy Cost function\n",
    "    J = (1/m)*(np.sum(losses))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Gradient <a anchor = \"anchor\" id = \"4.3\"></a>\n",
    "\n",
    "The Gradient is defined as follows: \n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{1}{m} \\big(\\sum_{i=1}^{m} { (y_{pred}^{(i)} - y^{(i)}) x^{(i)}_{j} \\big)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    '''\n",
    "    Usage:\n",
    "      #gradientDescent --> computes the gradient descent for linear regression\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #X --> The Design Matrix\n",
    "      #y --> The Ground Truth\n",
    "      #theta --> The Parameters which need to update\n",
    "      #alpha --> is the learning rate which indicates the learning step or how far we go down \n",
    "      #num_iters--> is the number of iterations needed to go to the global optima\n",
    "    \n",
    "    Returns:\n",
    "      #The updated parameters,theta \n",
    "      #cost_history: which is list containing the the values of the cost function, J, for every iteration\n",
    "    '''\n",
    "    #Compute m --> the number of training featue vectors\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Define the cost history as empty list\n",
    "    cost_history = []\n",
    "    \n",
    "    #Preallocating gradient for faster computaions \n",
    "    #The size of gradient equals:(numfeatures (includingx_0),)\n",
    "    dtheta = np.zeros((X.shape[1],))\n",
    "\n",
    "    \n",
    "    #Keep until Convergence\n",
    "    for i in range(num_iters+1):\n",
    "        \n",
    "        #Compute sigmoid of X𝜃 element-wise with the parameters, theta\n",
    "        h = sigmoid(X,theta)\n",
    "        \n",
    "        #dtheta is the partial derivates of cost function with respect to the parameters, theta\n",
    "        dtheta = (1/m)*(np.matmul(X.T, (np.subtract(h, y))))\n",
    "        \n",
    "        #Update theta\n",
    "        theta = theta - alpha*dtheta\n",
    "        \n",
    "        #While debugging, it can be useful to print out the values of the cost function (computeCost) \n",
    "        cost = computeCost(X,y,theta)\n",
    "        \n",
    "        #Append the value of the cost at a specific value for theta to cost_history\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        #print the cost function for every itration to track its new value step-by-step\n",
    "        print(\"Reached iteration: {0}, the cost = {1}\".format(i, cost))\n",
    "    \n",
    "    print(\"\\n\\nParameters have been trained!\") \n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "# Extracting the Features  <a anchor = \"anchor\" id = \"5\"></a>\n",
    "\n",
    "### Feature Extraction for a Single Tweet <a anchor = \"anchor\" id = \"5.1\"></a>\n",
    "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. \n",
    "* Then train your logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set. \n",
    "\n",
    "<b>After feature extraction , the feature will look like:</b>\n",
    "\n",
    "$$ x_m  = [1,\\sum_{w}{freqs(w,1)}\\hspace{1mm}, \\sum_{w}{freqs(w,0)}]$$\n",
    "\n",
    "<b>where,</b>\n",
    "\n",
    "$x_m:$ represents a feature corresponds to tweet $m$\n",
    "\n",
    "$1:$ represents the bias term \n",
    "\n",
    "$\\sum_{w}{freqs(w,1)}\\hspace{1mm}:$ epresents the sum of the positive frequencies for every unique word on the tweet $m$\n",
    "\n",
    "$\\sum_{w}{freqs(w,0)}\\hspace{1mm}:$ represents the sum of the negative frequencies for every unique word on the tweet $m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, frequency):\n",
    "    '''\n",
    "    Usage:\n",
    "      #extract_features --> used to extract features from a a given tweet to represent it \n",
    "      \n",
    "    Arguments:\n",
    "      #tweet --> list of tokens corresponds to a tweet\n",
    "      #frequency --> a dic mapping from (word, class) to corresponding frequency \n",
    "    \n",
    "    Returns:\n",
    "      #x --> 1 × 3 feature vector \n",
    "    '''\n",
    "    \n",
    "    #process the tweet by tokenizing, stemming, and removing stopwords\n",
    "    tokens_list= process_tweet(tweet)\n",
    "    \n",
    "    #pre-allocating a 1 × 3 feature vector \n",
    "    x = np.zeros((1,3))\n",
    "    \n",
    "    #adding the bias term \n",
    "    x[0,0] = 1\n",
    "    \n",
    "    #loop over every token in the list of tokens\n",
    "    for token in tokens_list:\n",
    "        #increment the word count corresponding to the positive class\n",
    "        x[0,1] += freqs.get((token,1),0)\n",
    "        \n",
    "        #increment the word count coressponing to the negative class\n",
    "        x[0,2] += freqs.get((token,0),0)\n",
    "        \n",
    "        \n",
    "    #to be safe, we will assert that the shape of our feature vector = (1, 3)\n",
    "    #if not, the program will stop, and give Assertion Error\n",
    "    assert(x.shape == (1,3))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code \n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra test \n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Feature Extraction for all the Tweets in the Training Set <a anchor = \"anchor\" id = \"5.2\"> </a>\n",
    "We want to apply the extract feature function to every tweet in our training set in order to construct the design matrix so we can training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDM(train_set, frequency):\n",
    "    '''\n",
    "    Usage:\n",
    "      #computeDM --> used to compute the design matrix for a given training set\n",
    "      \n",
    "    Arguments:\n",
    "      #train_Set --> list of tweets \n",
    "      #frequency --> a dic mapping from (word, class) to corresponding frequency \n",
    "    \n",
    "    Returns:\n",
    "      #X --> the design matrix for a given training set \n",
    "    '''\n",
    "    \n",
    "    #Pre-allocating the design matrix \n",
    "    X = np.zeros((len(train_set),3))\n",
    "    \n",
    "    #Loop over every tweet in the training set \n",
    "    for i in range(len(train_set)):\n",
    "        #Extract freature for the i-th tweet \n",
    "        X[i, :] = extract_features(train_set[i],frequency)\n",
    "        \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "# Training the Model <a anchor = \"anchor\" id = \"6\" > </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the design matrix \n",
    "X = computeDM(train_x, freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore X\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the shape fo X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the crossponding labels of the design matrix\n",
    "Y = train_y.reshape(8000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the shape of Y \n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "theta, cost_history =  gradientDescent(X,Y,theta=np.array([0,0,0]),alpha = 1e-9,num_iters = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the updated parameters\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Test your logistic regression <a anchor = \"anchor\" id = \"7\" ></a>\n",
    "\n",
    "$$ y_{pred} = Sigmoid(Z) = Sigmoid(\\theta^{T}X) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Usage:\n",
    "      #predict_tweet --> used to predict whether a tweet is positive or negative\n",
    "      \n",
    "    Arguments:\n",
    "      #tweet --> a string \n",
    "      #freqs --> a dic mapping from (word, class) to corresponding frequency\n",
    "      #theta --> the learned (updated) parameters\n",
    "    \n",
    "    Returns:\n",
    "      #y_pred--> the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(x,theta)\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your own tweet\n",
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Performance Using the Test Set <a anchor = \"anchor\" id = \"8\"></a>\n",
    "\n",
    "After training the model , we can test the how well the model does on unseen data to check its  performance.\n",
    "\n",
    "We can check its performance by computing the accuracy of the model as follows:\n",
    "\n",
    "$$ Accuracy = \\frac {Examples \\hspace{1mm} correctly \\hspace{1mm} classified} {Total \\hspace{1mm} number \\hspace{1mm} of \\hspace{1mm} examples}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    '''\n",
    "    Usage:\n",
    "      #test_logistic_regression --> used to compute the accuracy of the model \n",
    "      \n",
    "    Arguments:\n",
    "      #test_x --> a list of tweets\n",
    "      #test_y --> (m,) vector with the corresponding labels for the list of tweets\n",
    "      #freqs --> a dic mapping from (word, class) to corresponding frequency\n",
    "      #theta --> the learned (updated) parameters\n",
    "    \n",
    "    Returns:\n",
    "      #accuracy --> (# of tweets classified correctly) / (total # of tweets)\n",
    "    '''\n",
    "    \n",
    "    #Define m_test --> the number of testing examples\n",
    "    m_test = test_y.shape[0]\n",
    "    \n",
    "    #intialize a list for storing predictions \n",
    "    y_hat = []\n",
    "    \n",
    "    #loop over every tweet in the test set \n",
    "    for tweet in test_x:\n",
    "        \n",
    "        #get the estimated value of the true label y for every tweet \n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            #append 1 to y_hat \n",
    "            y_hat.append(1.0)\n",
    "            \n",
    "        else:\n",
    "            #append 0 to y_hat \n",
    "            y_hat.append(0.0)\n",
    "            \n",
    "        \n",
    "    #compute the accuracy of the model \n",
    "    accuracy = (np.sum(np.asarray(y_hat) == test_y) / m_test)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert test_y to rank one array so that the above function works properly \n",
    "Y_test = test_y.reshape(2000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the accuracy of our model \n",
    "tmp_accuracy = test_logistic_regression(test_x, Y_test, freqs, theta)\n",
    "print(tmp_accuracy)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis <a anchor = \"anchor\" id = \"9\"></a>\n",
    "\n",
    "In this part you will see some tweets that your model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does your model misclassify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with your Own Tweet <a anchor =\"anchor\" id = \"10\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(my_tweet, freqs, theta):\n",
    "    '''\n",
    "    Usage:\n",
    "      #pred--> used to predict whether a tweet is positive or negative\n",
    "      \n",
    "    Arguments:\n",
    "      #tweet --> a string  represents my own tweet\n",
    "      #freqs --> a dic mapping from (word, class) to corresponding frequency\n",
    "      #theta --> the learned (updated) parameters\n",
    "    \n",
    "    Returns:\n",
    "      #y_pred--> the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    #Compute the the probability of a tweet being positive \n",
    "    y_pred = predict_tweet(my_tweet, freqs, theta)\n",
    "    \n",
    "    if y_pred > 0.5:\n",
    "        print('Positive Sentiment')\n",
    "    else:\n",
    "        print('Negative Sentiment')\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict your own tweet\n",
    "my_tweet = \"What you must understand about me is that I’m a deeply unhappy person\"\n",
    "\n",
    "prediction = pred(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
