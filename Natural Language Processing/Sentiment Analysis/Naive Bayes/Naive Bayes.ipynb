{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis \n",
    "In this lab, we will build a sentiment analysis system which detects the attitude of the text . we build the system based on the Naive Bayes model. Naive Bayes is a probabilistic classifier , meaning that for a document $d$, out of all classes $c \\in C$ the classifier returns the class $ \\hat c$ which has the maximum posterior probability given the document. we express that mathematically as follows: $$ \\hat c = \\underset{c \\in C}{\\arg\\max}{P(c | d)} \\tag {1}$$ \n",
    "\n",
    "We use the formula of Bayes' rule to transform $Eq.1$ into some other probabilities that have some useful properties:\n",
    "\n",
    "$$ P(c|d) = \\frac {P(d|c) P(c)} {P(d)} \\tag{2}$$\n",
    "\n",
    "Bayes' rule gives us a way to break down any conditional probability, $P(c|d)$, into three other probabilities. We can dervie a new formula for $\\hat c$ using Bayes' rule as follows : \n",
    "$$ \\hat c = \\underset{c \\in C}{\\arg\\max}{P(c | d)} = \\underset{c \\in C}{\\arg\\max} {\\frac {P(d|c) P(c)} {P(d)}} \\tag {3} $$\n",
    "\n",
    "We can simplfy $Eq.3$ by dropping the denominator $P(d)$. This is possible since we compute $\\frac {P(d|c) P(c)} {P(d)}$ for each possible class. But $P(d)$ doesn't change for every class since we are always asking about the most likely class for the same document d, which must have the same probability $P(d)$. Thus, we can choos the class that maximizes the simpler formula: \n",
    "$$ \\hat c = \\underset{c \\in C}{\\arg\\max}{P(c | d)} = P(d|c) P(c) \\tag{4}$$\n",
    "\n",
    "As a result of this simplification , we can now compute the most probable class $\\hat c$ give some document d by choosing the class that has the highest product of two probabilities : \n",
    "\n",
    " - <b>Prior Probability $P(c)$: </b> represents what is originally believed about $c$ before new evidence is introduced or before collecting the document $d$ \n",
    "\n",
    " - <b> Likelihood $P(d|c)$: </b> The prbability of falling under a specific class \n",
    " \n",
    " \n",
    "We can simplify  $Eq.4$ by breaking viewing the document as a set of features $f_1, \\ldots , f_n$:\n",
    "$$ \\hat c = \\underset{c \\in C}{\\arg\\max}{P(c | d)} = P(f_1, \\ldots , f_n|c) P(c) \\tag{5}$$\n",
    "\n",
    "However, it still hard to compute $\\hat c$ using this formula due to the huge number of parameters we have in this equation. Moreover, We need to make some assumptions to simplfity the equation so we can compute $\\hat c$ easily. \n",
    "\n",
    "### Naive Bayes Assumption \n",
    " - <b>The Bag of Words Assumption</b>: we assume that the document $d$ is just a set of features (words) or a bag of features, $f_1, \\ldots , f_n$, regardless their order. So we assume that the features $f_1, \\ldots , f_n|c$ only encode the word identity and not the position \n",
    " - <b> Conditional Independence Assumption:</b> this is commonly known as Naive Bayes Assumption which we assume that the probabilities $P(f_i|c)$ are independent give the the class and based on that assumption we can break the likelihood into a set of simple probabilities as follows: $$P(f_1, \\dots , f_n|c)  = P(f_1|c) \\cdot P(f_2|c) \\cdots P(f_n|c) \\tag{6}$$\n",
    " \n",
    "\n",
    "based on these two assumptions we can re-write $Eq.5$ as follows: \n",
    "$$\\hat c = \\underset{c \\in C}{\\arg\\max}{P(c | d)} = \\underset{c \\in C}{\\arg\\max} P(f_1, \\ldots , f_n|c) P(c)  =  \\underset{c \\in C}{\\arg\\max} P(f_1|c) \\cdot P(f_2|c) \\cdots P(f_n|c) P(c) = \\underset{c \\in C}{\\arg\\max} P(c) \\prod_{f \\in F} {P(f|c)} \\tag{7}$$\n",
    "\n",
    "<b>Finally,</b> we can replace the set features by a set of words as the features that describe the document is just  a bunch of words. Thus, we can viewing the features $f_1, \\dots , f_n$ as a set of words $w_1, \\dots , w_n$ then the final equation chosen by a naive bayes classifirer is : $$\\hat c = \\underset{c \\in C}{\\arg\\max} P(c) \\prod_{i} {P(w_i|c)} \\tag{8} $$\n",
    "\n",
    "<b>However,</b> for a computational issue, Naive Bayes Calculations are done in log space to avoid underfolw and increase the \n",
    "computation speed. Thus $Eq.8$ is expressed as: \n",
    "\n",
    "$$ \\hat c = \\underset{c \\in C} {\\arg \\max} \\log{\\big (P(c) \\prod_{i} {P(w_i|c) \\big )}}  = \\underset{c \\in C} {\\arg \\max} \\log{P(c|d)} \\hspace{1mm} + \\hspace{1mm} \\sum_{i}{\\log{P(w_i|c)}} \\tag{9}\n",
    "$$\n",
    "\n",
    "#### Note: \n",
    "The classifiers that use a linear combination of the imputs to make a classification decision - like naive and also logistic regression are called <b>linear classifiers</b>\n",
    "\n",
    "# Outline \n",
    "* [Import Functions, Libraries, and Data](#1)\n",
    "* [Pre-process The Data](#2)\n",
    " * [Remove Noise](#3)\n",
    " * [Building a Frequency Table](#4)\n",
    " * [Computing the Positive and Negative Frequencies for a Word in a Certain Class](#44)\n",
    " * [Pre-processing Step](#5)\n",
    "* [Training the Model](#6)\n",
    "* [Testing the Model](#7)\n",
    " * [ Make Predictions](#71)\n",
    " * [Compute the Accuracy](#72)\n",
    "* [Error Analysis](#8)\n",
    "\n",
    "\n",
    "\n",
    "# Import Functions, Libraries, and Data <a anchor = \"anchor\" id = \"1\" > </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries and functions\n",
    "import numpy as np \n",
    "import math \n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlaod the data needed for that lab\n",
    "#nltk.download(\"twitter_samples\")\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data \n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pre-process The Data <a anchor = \"anchor\" id = \"2\"></a>\n",
    "\n",
    "\n",
    "\n",
    "## Remove Noise  <a anchor = \"anchor\" id = \"3\"></a>\n",
    "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
    "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
    "- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n",
    "- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
    "- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Usage:\n",
    "      #process_tweet --> used to clean the text, tokenize it into separate words, remove stopwords, \n",
    "                         and convert words to stems.\n",
    "      \n",
    "    Arguments:\n",
    "      #tweet --> a string containing  a tweet \n",
    "    \n",
    "    Returns:\n",
    "      #tweet_clean --> a list of words containing  the processed tweet\n",
    "    '''\n",
    "    \n",
    "    #create a new Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    #get the latest of all the English stop words \n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #define an empty list which will hold the cleaned tokens\n",
    "    tweets_clean = []\n",
    "    \n",
    "    #loop over every tokens in the list of tokens, tweets_tokens\n",
    "    for token in tweet_tokens:\n",
    "        if (token not in stopwords_english and token not in string.punctuation):\n",
    "            \n",
    "            #stemming that token \n",
    "            stem_token = stemmer.stem(token)\n",
    "            \n",
    "            #append the stem of the token in the tweets_clean list\n",
    "            tweets_clean.append(stem_token)\n",
    "            \n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(process_tweet(custom_tweet)).encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Frequency Table <a anchor = \"anchor\" id =\"4\" ></a>\n",
    "To help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    '''\n",
    "    Usage:\n",
    "      #bulid_freqs --> used to count how often a word in the 'corpus' (the entire set of tweets) was \n",
    "                       associated with a positive label '1' or a negative label '0', then builds \n",
    "                       the freqs dictionary, where each key is a (word,label) tuple, \n",
    "                       and the value is the count of its frequency within the corpus of tweets.\n",
    "      \n",
    "    Arguments:\n",
    "      #tweets --> a list of tweets \n",
    "      #ys --> a m x 1 array holds the sentiment label or the class (0 or 1) corresponds to every tweet\n",
    "    \n",
    "    Returns:\n",
    "      #freqs--> a dictionary whose key is (word, sentiment label (class)) and whose value frequency\n",
    "                     of the word which is the number of times that word showes up in that class\n",
    "    '''\n",
    "    \n",
    "    #reduce the rank of the ys array to be rank one array, then, convert it to list \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    \n",
    "    #initialize freqs dic as empty dic which will be populated by looping over every tweet in tweets\n",
    "    freqs = {}\n",
    "    \n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            \n",
    "            #if the key exist in the dic, increment the value by one \n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            #if not, intialize its value to one \n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "\n",
    "build_freqs(tweets,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Positive and Negative Frequencies for a Word in a Certain Class<a anchor = \"anchor\" id =\"44\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up(freqs, word, label):\n",
    "    '''\n",
    "    Usage:\n",
    "      #train_naive_naive_bayes --> used for training the model\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #freqs --> a dic which map each (word, label) to its corresponding frequency\n",
    "      #word --> the word to look up\n",
    "      #label: the label corresponding to the word\n",
    "    \n",
    "    Returns:\n",
    "      #n --> the number of times the word of interest appears in all the documents of topic c (c= label)\n",
    "    \n",
    "    Notes:\n",
    "      #We return zero if the tuple doesn't exist in the frequency table \n",
    "    '''\n",
    "    \n",
    "    #initialize n \n",
    "    n = 0\n",
    "    \n",
    "    #get the given 2-tuple\n",
    "    pair = (word, label) \n",
    "    \n",
    "    #check if that tuple exists in the frequency table \n",
    "    if (pair in freqs):\n",
    "        \n",
    "        n = freqs[pair] #get the corresponding frequency to this tuple\n",
    "        \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Step  <a anchor = \"anchor\" id = \"5\"></a>\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
    "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#select the set of positive and negative tweets \n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "#split the data into train and test set \n",
    "train_pos = all_positive_tweets[ :4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "\n",
    "#concatenating the training tweets, positive and negative \n",
    "train_x  = train_pos + train_neg \n",
    "\n",
    "#conatenating the testing tweets, positive and negatives\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the training tweets \n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the test tweets\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the negative and postive labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore train_y \n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore test_y \n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create frequency dictionary \n",
    "freqs = build_freqs(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the freq dictionary \n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the length of the frequency dic \n",
    "len(freqs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model <a anchor = \"anchor\" id = \"6\"></a>\n",
    "\n",
    "The goal of training the model is to learn the probabilities $P(c)$ and $P(w_i,c)$ but how ?. We simply use the frequencies in the data.\n",
    "\n",
    "\n",
    "<b>For</b> $P(c)$:\n",
    "We ask the precentage of the documents in our training set are in each class $c$. Thus the prior probability is given by : \n",
    "\n",
    "$$P(c) = \\frac {N_c} {N_{doc}}$$  where, \n",
    "\n",
    "$N_c:$ the number of the documents in our training set falling under the class $c$\n",
    "\n",
    "$N_{doc}:$ the total number of documents \n",
    "\n",
    "<b>For</b> $P(w_i|c)$:\n",
    "We compute the likelihood as the fration of times the word $w_i$ appears among all the words in all documents of topic $c$. Thus the likelihood is given by : \n",
    "$$ P(w_i|c) = \\frac {count(w_i, c)} { \\sum_{w \\in V}{count(w,c)}}$$\n",
    "\n",
    "where, \n",
    "\n",
    "$count(w_i, c):$ the number of times the word $w_i$ appears in all documents of topic $c$ \n",
    "\n",
    "$\\sum_{w \\in V}{count(w,c)}:$ the sum of the frequency of each word in the documents of topic $c$\n",
    "\n",
    "\n",
    "#### ِAdd-One Smoothing\n",
    "There's a problem with training the likelihood, imagine that we are trying to estimate the word \"great\" given class positive, but suppose that our training set is not enough to hold all words so the word great doesn't appear in the documents so the it has a frequency of zero and thus the the $P(w_i, c)$ is equal to zero since the numerator equals zero and that affects the classification as a whole since the naive bayes model multiplies all the feature likelihoods together which lead to zero probability. one of the solutions that we can use is <b>add-one smoothing</b>. We pretend that the frequency of every word in the vocabulary is incremented by one and thus the likelihood is given by:\n",
    "$$ P(w_i|c) = \\frac {count(w_i, c) + 1} { \\sum_{w \\in V}{(count(w,c) + 1 )}} =  \\frac {count(w_i, c) + 1} {\\big ( \\sum_{w \\in V}{count(w,c)}\\big) + |V|}$$\n",
    "\n",
    "\n",
    "\n",
    "#### The Pseudo-code of The Training Algorithm\n",
    "<img src = \"https://i.imgur.com/26TEwqH.png\" width = \"50%\" >\n",
    "\n",
    "\n",
    "\n",
    "#### We will make some modifications in the algorithm: \n",
    "\n",
    " - Instead of computing $P(d|c = -)$ and $P(d|c = +)$ and compare them to find what is more probable, we will divide them.\n",
    " - By convention , we divide the $P(d|c = +)$ by $P(d|c = -)$\n",
    " - If the quotient > 0 the the positive class is more probable  than the negative one \n",
    " - If the quatient < 0 the negative class is more probable than the positive one \n",
    " - The formula of that division is as follows :\n",
    " $$ \\frac{P(d|+)}{P(d|-)} = \\log\\bigg (\\frac{P(+)} {P(-)} \\cdot \\prod_{i}\\big( \\frac{P(w_i|+)}{P(w_i|-)} \\big)\\bigg)  = \\log\\big (\\frac{P(+)} {P(-)}\\big) + \\sum_{i}{\\log\\bigg({\\frac{P(w_i|+)}{P(w_i|-)}}\\bigg)} = log\\hspace{1mm}prior + log\\hspace{1mm}likelihood\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "      #train_naive_naive_bayes --> used for training the model\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #freqs --> a dic which map each (word, label) to its corresponding frequency\n",
    "      #train_x --> a list of tweets \n",
    "      #train_y -->  a list of labels correponding to the tweets (0,1)\n",
    "    \n",
    "    Returns:\n",
    "      #logprior --> log𝑃(𝑐)\n",
    "      #loglikelihood --> a dictionary with the loglikelihoods for each word --> {W_1: log𝑃(𝑤1|𝑐),..., W_n: log𝑃(𝑤n|𝑐)}\n",
    "    '''\n",
    "    \n",
    "    #initialize the loglikelihood dic as well as logprior\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    #get the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    \n",
    "    #compute the vocabulary size\n",
    "    V = len(vocab)\n",
    "    \n",
    "    #initialize Npos and Nneg \n",
    "    N_pos = N_neg = 0\n",
    "    \n",
    "    #compute  Npos and Nneg -->  ∑𝑐𝑜𝑢𝑛𝑡(𝑤,𝑐)\n",
    "    #Loop over every (word, label) in the frequency table\n",
    "    for pair in freqs.keys():\n",
    "        \n",
    "        #if the label is positive (greater than zero) \n",
    "        if pair[1] > 0:\n",
    "            \n",
    "            #increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            N_neg += freqs[pair]\n",
    "            \n",
    "    \n",
    "    #########################\n",
    "    # compute the log prior #\n",
    "    ########################\n",
    "    \n",
    "    #calculate the total number of documents \n",
    "    D = len(train_y)\n",
    "    \n",
    "    #compute the number of positive documents \n",
    "    D_pos = np.squeeze(sum(filter(lambda X: X > 0, train_y)))\n",
    "    \n",
    "    #compute the number of negative documents \n",
    "    D_neg = D - D_pos\n",
    "    \n",
    "    \n",
    "    #Calculate the log prior \n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    ##############################\n",
    "    # compute the log likelihood #\n",
    "    ##############################\n",
    "    \n",
    "    \n",
    "    #Loop over each word in the vocabulary\n",
    "    for word in vocab:\n",
    "        \n",
    "        #get the positive and negative frequencies of the word \n",
    "        freqs_pos = look_up(freqs, word, 1) # 𝑐𝑜𝑢𝑛𝑡(𝑤_i,+)\n",
    "        freqs_neg = look_up(freqs, word, 0) # 𝑐𝑜𝑢𝑛𝑡(𝑤_i,-)\n",
    "        \n",
    "        #calculate 𝑃(𝑤ord|+) \n",
    "        p_w_pos = (freqs_pos + 1) / (N_pos + V)\n",
    "        \n",
    "        #calculate 𝑃(𝑤ord|-) \n",
    "        p_w_neg = (freqs_neg + 1) / (N_neg + V)\n",
    "        \n",
    "        #calculate the log likelihood of the word \n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test the code\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model <a anchor = \"anchor\" id = \"7\"></a>\n",
    "\n",
    "#### The Pseudo-code of The Testing Algorithm\n",
    "\n",
    "<img src = \"https://i.imgur.com/c0Z36Yh.png\" width = \"800px\">\n",
    "\n",
    "\n",
    "### Note:\n",
    "- Because we make some modification on training algorithm that affect the testing algorithm we will make as instead of outputing the highest probability given the document we want to test , we will outputing the fraction that tell us that document is more likely to fall in the positive class or the negative one \n",
    "- If the quotient > 0 the the positive class is more probable  than the negative one \n",
    "- If the quatient < 0 the negative class is more probable than the positive one \n",
    "- The log prior could be a ngeative value, ex(if the fraction is between 0 and one --> log(0.5) = -0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Make Predictions <a anchor = \"anchor\" id = \"71\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Usage:\n",
    "      #train_naive_naive_bayes --> used for training the model\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #tweet --> the tweet or the document the need to test \n",
    "      #logprior --> log𝑃(𝑐)\n",
    "      #loglikelihood --> a dictionary with the loglikelihoods for each word --> {W_1: log𝑃(𝑤1|𝑐),..., W_n: log𝑃(𝑤n|𝑐)}\n",
    "    \n",
    "    Returns:\n",
    "      #p --> the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    '''\n",
    "    \n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            \n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test\n",
    "\n",
    "my_tweet = 'She is sad.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Accuracy <a anchor = \"anchor\" id = \"72\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    '''\n",
    "    Usage:\n",
    "      #train_naive_naive_bayes --> used for training the model\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #test_x --> list of tweets\n",
    "      #test_y --> the corresponding labels for the list of tweets\n",
    "      #logprior --> log𝑃(𝑐)\n",
    "      #loglikelihood --> a dictionary with the loglikelihoods for each word --> {W_1: log𝑃(𝑤1|𝑐),..., W_n: log𝑃(𝑤n|𝑐)}\n",
    "    \n",
    "    Returns:\n",
    "      #accuracy --> (# of tweets classified correctly)/(total # of tweets)\n",
    "    '''\n",
    "    \n",
    "    #initialize the accuracy \n",
    "    accuracy = 0\n",
    "    \n",
    "    #define an empty list ,y_hat, which will hold the list of the estimated value of y for every tweet in test_x list \n",
    "    y_hats = []\n",
    "    \n",
    "    \n",
    "    #Loop over every tweet in the list of tweets, test_x\n",
    "    for tweet in test_x:\n",
    "        \n",
    "        #if prediction > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat_i = 1\n",
    "        else :\n",
    "            y_hat_i = 0\n",
    "        \n",
    "        #append the estimated value of y_hat coresponding to this tweet \n",
    "        y_hats.append(y_hat_i)\n",
    "        \n",
    "    #convert y_hat to columns vector to get the value of mean properly\n",
    "    y_hats = np.array(y_hats).reshape(test_y.shape)\n",
    "        \n",
    "    \n",
    "    #compute the error which is (# of tweets classified icorrectly)/(total # of tweets)\n",
    "    error = np.mean(np.absolute(y_hats - test_y))\n",
    "    \n",
    "    #compute the accuracy which is 1-error \n",
    "    accuracy = 1 - error\n",
    "    \n",
    "        \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis <a anchor = \"anchor\" id = \"8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0): # or (y_hat > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (y_hat > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about <b>encode() function</b> visit [encode()](https://www.w3schools.com/python/ref_string_encode.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
