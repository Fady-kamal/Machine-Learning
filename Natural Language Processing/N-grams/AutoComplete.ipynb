{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models: Auto-Complete\n",
    "\n",
    "In this lab, we will build an auto-complete system.Auto-complete system is a system which gives the user options or suggestions to complete the sentence.\n",
    "\n",
    "## Applications of  Auto-complete\n",
    "\n",
    "- When you google something, you often have suggestions to help you complete your search. \n",
    "- When you are writing an email, you get suggestions telling you possible endings to your sentence.  \n",
    "\n",
    "<img src = \"https://i.imgur.com/OMbdb82.png\" style=\"width:700px;height:300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## How to build an Auto-complete system ?\n",
    "To build an Auto-complete system, you need a language model. A language model is is a probability distribution over sequences of words.In other words, it assigns a probability $P(w_{1},\\ldots,w_{m})$ to the whole sequence.\n",
    "\n",
    "\n",
    "One of the langauge model that we use to build an Auto-complete syetm is <strong>N-gram model</strong> which predicts the probability of word $w$ given a history of words $h$ $P(w|h)$.\n",
    "\n",
    "<strong>Ex:</strong> Suppose the history $h$ is <strong>\"its water is so transparent so that\"</strong>\n",
    "and we want to know the probability that the next word is <strong>the:</strong>\n",
    "\n",
    "<p style =\"text-align: center\">$\\large P(the\\hspace{1mm} |\\hspace{1mm} its\\hspace{1mm} water\\hspace{1mm}  is \\hspace{1mm} so\\hspace{1mm}  transparent \\hspace{0.4mm} so\\hspace{1mm} that)$</p>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "# Outline\n",
    "* [Import the necessary libraries and the data](#0)\n",
    "* [Explore the data](#1)\n",
    "* [Pre-process the data](#2)\n",
    " * [Split the data into sentences](#3.1)\n",
    " * [Tokenize sentences](#3.2)\n",
    " * [Putting all togther ](#3.3)\n",
    " * [Split the data into train and test sets](#3.4)\n",
    " * [Find tokens that appear at least N times in the training data](#3.5)\n",
    " * [Replace tokens that appear less than N times by < unk >](#3.6)\n",
    " * [Putting all together](#3.7)\n",
    " * [Pre-process the train and test set](#3.8)\n",
    "* [Develop n-gram model](#4)\n",
    " * [Compute the counts of n-grams for an arbitrary number  ùëõ](#4.1)\n",
    " * [Estimate the probability of a word given some history of words ](#4.2)\n",
    " * [Estimate probabilities for all words](#4.3)\n",
    " * [Visualize n-gram couts in a matrix form ](#4.4)\n",
    " * [Visualize n-gram probabilities in a matrix form](#4.5)\n",
    "* [Perplexity](#5)\n",
    "* [Build an auto-complete system](#6)\n",
    " * [Suggest A word](#6.1)\n",
    " * [Suggest Multiple Words](#6.2)\n",
    " * [Suggest Multiple Words Using n-grams of Varying Length](#6.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries and the data <a anchor = \"anchor\" id = \"0\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessay library\n",
    "import math\n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data \n",
    "with open(\"en_US.twitter.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data <a anchor = \"anchor\" id = \"1\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the datatype \n",
    "print(\"Data type:\", type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the length of the data --> (The number of letters)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display subset of the data --> the first 300 letters in the data\n",
    "display(data[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display subset of the data --> the last 300 letters in the data\n",
    "display(data[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data <a anchor = \"anchor\" id = \"2\" />\n",
    "Pre-processing the data is to prepare the data to feed it to our model by converting the raw data to meaningful one.\n",
    "\n",
    "<strong>The steps to pre-process the data are as follows:</strong>\n",
    "\n",
    "* Split data into sentences using \"\\n\" as the delimiter.\n",
    "* Split each sentence into tokens. Note that in this assignment we use \"token\" and \"words\" interchangeably.\n",
    "* Assign sentences into train or test sets.\n",
    "* Find tokens that appear at least N times in the training data.\n",
    "* Replace tokens that appear less than N times by `<unk>`\n",
    "\n",
    "\n",
    "## Split the data into sentences <a anchor = \"anchor\" id = \"3.1\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):  \n",
    "    '''\n",
    "    Usage:\n",
    "      #split_to_senteces--> used for splitting data by linebreak \"\\n\"\n",
    "  \n",
    "    Arguments:\n",
    "      #data --> string\n",
    "    \n",
    "    Returns:\n",
    "      #sentences --> a list of sentences\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Split when you find a linebreak ,\"\\n\", in the data\n",
    "    sentences = data.split('\\n')\n",
    "    \n",
    "    #Remove leading from each sentence --> a leading space to appear at the beginning of the character\n",
    "    #and trailing spaces from each sentece --> a trailing space to appear at the end of the character\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    #Drop sentences if they are empty strings --> \"\"\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "txt = \"   I love Aswan\\nI love Luxor\"\n",
    "print(\"Before:\\n\")\n",
    "print(txt)\n",
    "print(\"------\")\n",
    "print(\"After:\")\n",
    "split_to_sentences(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "\n",
    "x = \"\"\"\n",
    "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
    "\"\"\"\n",
    "print(x)\n",
    "\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sentences <a anchor = \"anchor\" id = \"3.2\" />\n",
    "\n",
    "After splitting the data into sentences, the next step is to tokenize the senteces, or in other words, to split every sentence into list of words called tokens.\n",
    "\n",
    "<strong>The steps to tokenize sentences:</strong>\n",
    "\n",
    "- Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.\n",
    "- Append each tokenized list of words into a list of tokenized sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    '''\n",
    "    Usage:\n",
    "      #sentences--> used for to split every sentence into list of words.\n",
    "      \n",
    "    Arguments:\n",
    "      #sentences --> list of strings (strings)\n",
    "    \n",
    "    Returns:\n",
    "      #tokenized_sentences --> a list of lists of tokens\n",
    "    '''\n",
    "    \n",
    "    #Initialize empty list that will hold the lists of tokens \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    #Loop over every sentence in sentences\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        #Convert the sentence to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        #Then, convert it to list of words (tokens)\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        \n",
    "        #Append the list of word, tokenized, to the list of lists, tokenized_sentences\n",
    "        tokenized_sentences.append(tokenized)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "txt = [\"I love Aswan\", \"I love Luxor\"]\n",
    "print(\"Before:\\n\")\n",
    "print(txt)\n",
    "print(\"------\")\n",
    "print(\"After:\")\n",
    "tokenize_sentences(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all togther <a anchor = \"anchor\" id = \"3.3\" />\n",
    "\n",
    "Use the two functions that you have just implemented to get the tokenized data.\n",
    "- split the data into sentences\n",
    "- tokenize those sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    '''\n",
    "    Usage:\n",
    "      #get_tokenized_data--> used for tokenizing the data to become list of lists of tokens\n",
    "  \n",
    "    Arguments:\n",
    "      #data --> string\n",
    "    \n",
    "    Returns:\n",
    "      #tokenized_sentences --> --> a list of lists of tokens\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Split the data into sentences\n",
    "    sentences = split_to_sentences(data)\n",
    "    \n",
    "    #Tokenize the data into a list of lists of tokens\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "txt = \"   I love Aswan\\nI love Luxor\"\n",
    "print(\"Before:\\n\")\n",
    "print(txt)\n",
    "print(\"------\")\n",
    "print(\"after\")\n",
    "get_tokenized_data(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test\n",
    "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets <a anchor = \"anchor\" id = \"3.4\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, tokenize the data\n",
    "tokenized_data = get_tokenized_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-order the lists of tokens inside the list tokenized_data\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 80%  train set and 20% test set\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the length of the train and test set relative to the data\n",
    "print(f\"The tokenized data length: {len(tokenized_data)}\\nThe train set length: {len(train_data)}\\nThe test set length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the first example in train set \n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the first example in the test set\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find tokens that appear at least N times in the training data <a anchor = \"anchor\" id = \"3.5\" />\n",
    "\n",
    "\n",
    "- we won't use all the tokens (words) appearing in the data for training. Instead, we will use the more frequently used words.\n",
    "\n",
    "- we will focus on the words that appear at least N times in the data.\n",
    "\n",
    "- First count how many times each word appears in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    '''\n",
    "    Usage:\n",
    "      #count_words --> used for Count the number of word appearence for every tokenized sentence in the tokenized sentences\n",
    "  \n",
    "    Arguments:\n",
    "      #tokenized_sentences --> list of lists of tokens\n",
    "    \n",
    "    Returns:\n",
    "      #word_counts --> dict that maps word (str) to the frequency (int)\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Define empty count which will hold the (key,value) pairs which represent (word,frequency) for every word\n",
    "    word_counts = {}\n",
    "    \n",
    "    #Loop over every sentence in the tokenized_sentences list\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        #Loop over every token in the sentence\n",
    "        for token in sentence:\n",
    "            \n",
    "            #If the token is not in the dictionary yet, set the count to 1\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            \n",
    "            #If the token is already in the dictionary, increment the count by 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "                \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "sentences = \"   I love Aswan\\nI love Luxor\"\n",
    "tokenized_sentences = get_tokenized_data(sentences)\n",
    "print(f'The tokenized sentences: {txt}\\n')\n",
    "print(f'The frequency of every word {count_words(tokenized_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace tokens that appear less than N times by   `<unk>` <a anchor = \"anchor\" id = \"3.6\" />\n",
    "\n",
    "If your model is performing autocomplete, but encounters a word that it never saw in therir vocabulary, it won't have some hsitory of words to help it determine the next word to suggest. As a result of that, the model won't able to predict the next word.\n",
    "\n",
    "- This 'new' word is called an 'unknown word', or <b>out of vocabulary (OOV)</b> words.\n",
    "- <b>Out-of-vocabulary (OOV) words</b> are unknown words that appear in the testing speech but not in the recognition vocabulary.\n",
    "- The percentage of unknown words in the test set is called the <b> OOV </b> rate. \n",
    "\n",
    "<strong>To handle unknown words during prediction, use a special token to represent all unknown words 'unk'.</strong>\n",
    "- Modify the training data so that it has some 'unknown' words to train on.\n",
    "- Words to convert into \"unknown\" words are those that do not occur very frequently in the training set.\n",
    "- Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. \n",
    "- Convert all the other words that are not part of the closed vocabulary to the token 'unk'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    '''\n",
    "    Usage:\n",
    "      #get_words_with_nplus_frequency --> used for finding the words that appear N times or more\n",
    "                                          to hold them in the vocabulary list\n",
    "                                          \n",
    "    Arguments:\n",
    "      #tokenized_sentences --> list of lists of tokens\n",
    "      #count_threshold --> cut-off value to sepreate the closed vocabulary list form the ouf of the vocabulary list\n",
    "    \n",
    "    Returns:\n",
    "      #closed_vocab --> represents the closed vocabulary list that contain the words that appear N times or more\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Initialize an empty list that will hold the words that apprear at least N times\n",
    "    closed_vocab = []\n",
    "    \n",
    "    #Get the count of every word  in the form of (word, frequency) pair\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    #Loop over every tuple holds the (word,frequency) pair\n",
    "    for word,cnt in word_counts.items():\n",
    "        #if the count of the word at least is equal to count_threshold\n",
    "        if cnt >= count_threshold:\n",
    "            \n",
    "            #append the word to the closed vocabulary list \n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "sentences = \"   I love Aswan\\nI love Luxor\"\n",
    "tokenized_sentences = get_tokenized_data(sentences)\n",
    "print(f'The tokenized sentences: {txt}\\n')\n",
    "print(f'The frequency of every word {count_words(tokenized_sentences)}\\n')\n",
    "closedVocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"The closed vocabulary list: {closedVocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    '''\n",
    "    Usage:\n",
    "      #replace_oov_words_by_unk --> used to replace words not in the closed vocabulary with the token <unk>.\n",
    "                                          \n",
    "    Arguments:\n",
    "      #tokenized_sentences --> list of lists of tokens\n",
    "      #vocabulary --> represents the closed vocabulary list that contain the words \n",
    "                      that appear N times or more\n",
    "      #unknown_token --> string represents out-of-vocabulary words\n",
    "    \n",
    "    Returns:\n",
    "      #replaced_tokenized_sentences --> list of lists of tokens with out-of vocabulary words\n",
    "                                        replaced by <unk>\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #convert the list into set to remove the repeated words \n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    #intialize a list which will hold lists of tokens with out-of vocabulary words replaced by <unk>\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    #Loop over every sentence in the tokenized_sentences list\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        #initialize a list that represent a single list from \n",
    "        #the lists in the list replaced_tokenized_sentences\n",
    "        replaced_sentence = []\n",
    "        \n",
    "        #Loop over every token in the sentence\n",
    "        for token in sentence:\n",
    "            \n",
    "            #check if the token in the closed vocab list\n",
    "            if token in vocabulary: # complete this line\n",
    "                # If so, append the word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                # otherwise, append the unknown token instead\n",
    "                replaced_sentence.append(unknown_token)\n",
    "            \n",
    "            \n",
    "        #append the single list, replaced_sentence, to the list of lists, replaced_tokenized_sentences\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "sentences = \"   I love Aswan\\nI love Luxor\"\n",
    "tokenized_sentences = get_tokenized_data(sentences)\n",
    "print(f'The tokenized sentences: {txt}\\n')\n",
    "print(f'The frequency of every word {count_words(tokenized_sentences)}\\n')\n",
    "closedVocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"The closed vocabulary list: {closedVocab}\\n\")\n",
    "\n",
    "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, closedVocab)\n",
    "print(f\"The replaced tokenized sentences: {tmp_replaced_tokenized_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
    "vocabulary = [\"dogs\", \"sleep\"]\n",
    "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
    "print(f\"Original sentence:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together <a anchor = \"anchor\" id = \"3.7\" />\n",
    "\n",
    "We can encapsualte what we have done above in a single function in order to preprocess the data\n",
    "\n",
    "<b> We will do the following:</b>\n",
    "1. Find tokens that appear at least count_threshold times in the training data.\n",
    "1. Replace tokens that appear less than count_threshold times by \"<unk\\>\" both for training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold = 2):\n",
    "    '''\n",
    "    Usage:\n",
    "      # preprocess_data:\n",
    "         -Find tokens that appear at least count_threshold times in the training data.\n",
    "         -Replace tokens that appear less than N times by \"<unk>\" both for training and test data. \n",
    "                                          \n",
    "    Arguments:\n",
    "      #train_data, test_data --> each of them is a list of lists of tokens\n",
    "      #count_threshold --> cut-off value to sepreate the closed vocabulary list form the ouf of the vocabulary list\n",
    "                           the default value is 2\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #train_data_replaced --> training data with low frequent words replaced by \"<unk>\"\n",
    "      #test_data_replaced --> test data with low frequent words replaced by \"<unk>\"\n",
    "      #closed_vocab --> represents the closed vocabulary list\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Get the closed vocabulary list from the training data\n",
    "    closed_vocab = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    \n",
    "    #For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data,closed_vocab)\n",
    "    \n",
    "    #For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data,closed_vocab)\n",
    "    \n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "sentences1 = \"   I love Aswan\\nI love Luxor\"\n",
    "sentences2 = \"   I love china\\n I love Jaban\"\n",
    "tokenized_tr = get_tokenized_data(sentences1)\n",
    "tokenized_test = get_tokenized_data(sentences2)\n",
    "temp_train, temp_test, temp_closed_vocab = preprocess_data(tokenized_tr, tokenized_test)\n",
    "\n",
    "print(f\"pre-processed train data: {temp_train}\\npre-processed test data: {temp_test}\\nThe closed vocabulary list: {temp_closed_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "# test your code\n",
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
    "                                                           tmp_test, \n",
    "                                                           count_threshold = 1)\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Pre-process the train and test set <a anchor = \"anchor\" id = \"3.8\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process the data\n",
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the first processed list (sentence) in the processed training set \n",
    "train_data_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the first processed list (sentence) in the processed test set \n",
    "test_data_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore subset of the closed vocabulary list \n",
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the size of the closed vocabulary list \n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop n-gram model <a anchor = \"anchor\" id = \"4\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Compute the counts of n-grams for an arbitrary number  ùëõ  <a anchor = \"anchor\" id = \"4.1\" />\n",
    "\n",
    "The term <b>n-gram</b> referes to two things: \n",
    " - the n-gram model\n",
    " - a squence of n words\n",
    " \n",
    "So , when we say the counts of n-grams , we mean the count of every n-word sequence in a given data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "      #count_n_grams --> used for counting all n-grams in the data  \n",
    "                                          \n",
    "    Arguments:\n",
    "      #data --> represents list of lists of tokens\n",
    "      #n --> represents the number of words in a sequence (n-gram)\n",
    "      #start_token --> a token indicating the beginning of a sentence\n",
    "      #end_token --> a token indicating the end of a sentence\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #n_grams --> a dic whose key is the n-gram(a sequence of n words) represented in tuple, \n",
    "                   and whose value is the counts of every n-gram\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Initialize a dictionary holds n-gram and their counts \n",
    "    n_grams = {}\n",
    "    \n",
    "    #Loop over every sentence (List) in data (list of Lists)\n",
    "    for sentence in data:\n",
    "        \n",
    "        #add n-start_tokens for every sentence based on the type of n-grams (ex: n = 2 for bigrams)\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "        \n",
    "        #Convert sentence (list) to tuple\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        #compute the number of words in sentence to loop over them to compute their counts\n",
    "        m = len(sentence) if n == 1 else len(sentence) - 1\n",
    "        \n",
    "        #Loop every token in sentence except the last one as we key the n-gram from i to i+n\n",
    "        #the index i represents the starting point of every n-gram\n",
    "        for i in range(m):\n",
    "            \n",
    "            #get the n-gram from i to i+n\n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            # check if the n-gram is in the dictionary\n",
    "            if n_gram in n_grams.keys():\n",
    "            \n",
    "                #increment the count (the value of the key) for this n-gram\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                #add this n-gram as a key in the dictionary follwed by its value (their count)\n",
    "                n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "sentences = [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "\n",
    "print(\"Uni-gram\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"\\nBi-gram:\")\n",
    "print(count_n_grams(sentences, 2))\n",
    "print(\"\\nTri-gram\")\n",
    "print(count_n_grams(sentences, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Estimate the probability of a word given some history of words <a anchor = \"anchor\" id = \"4.2\" /> \n",
    "\n",
    "\n",
    "The n-gram model is estimated , using <b>Markov Assumption</b>, as :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat P(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "But unfortunatley that there are many n-grams that are not necessarily in the training set, so their counts will be equal to zero, and therefore their pobabilities also will be zero.\n",
    "\n",
    "So we need a solution to this problem. One of the solution is <b>add-k smoothing</b>. we pretend that every token in our vocabulary  is incremented by one, therefore, every n-gram is incremented by one \n",
    ". so the formula of the n-gram model will be: \n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "This means that any n-gram with zero count has a probability of $\\frac{1}{|V|}$ where $|V|$ is the vocabulary size .\n",
    "\n",
    "\n",
    "\n",
    "### Notes:\n",
    "\n",
    "* $C(w_{t-1}\\dots w_{t-n}):$ represents (in the below function) the previous n-gram counts\n",
    "\n",
    "* $C(w_{t-1}\\dots w_{t-n}, w_n) :$ represents (in the below function) the n-plus-one-gram counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    '''\n",
    "    Usage:\n",
    "      #estimate_probability --> used to Estimate the probability of a next word using \n",
    "                                the n-gram counts with k-smoothing \n",
    "                                          \n",
    "    Arguments:\n",
    "      #word --> represents the next word we want to estimate its probability\n",
    "      #previous_n_gram -->  a sequence of n words\n",
    "      #n_gram_counts --> the number of times the history of the words appears together \n",
    "      #n_plus1_gram_counts --> the number of times the history of the words and the next word appear together\n",
    "      #vocabulary_size --> the number of words in the vocabulary\n",
    "      #K --> represents the smoothing parameter which is  the value of K for add-k somthing technique\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #probability --> representes the estimated proabaility of a next word \n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Convert the previous n-gram list to tuple to use it as a dictionary key \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    # Set the denominator #\n",
    "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
    "    # Get its count.  Otherwise set the count to zero\n",
    "    # Use the dictionary that has counts for n-grams\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "    \n",
    "    #Compute the denominator using add-k smoothing technique\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    \n",
    "    # Set the numerator #\n",
    "    #Define the n-plus-one-gram by appending the next word in the previous n-gram\n",
    "    n_plus1_gram = previous_n_gram + (word, )\n",
    "    \n",
    "    # Set the count to the count in the dictionary,\n",
    "    # otherwise 0 if not in the dictionary\n",
    "    # use the dictionary that has counts for the n-gram plus current word\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "    \n",
    "    #Compute the numerator\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    \n",
    "    # Compute the probability of the next word #\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code\n",
    "sentences = sentences = [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1) #previous n-gram counts \n",
    "bigram_counts = count_n_grams(sentences, 2)  #n-plus-one-gram counts\n",
    "\n",
    "#Estimate the probaility of being \"cat\" given the previous word \"a\" using the bigram model\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The 1/|V| value: {1/len(unique_words)}\")\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As you see,</b> both of the words \"a\" and \"cat\" don't exist in the training set so we give them a proability of $ \\frac{1}{|V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate probabilities for all words <a anchor = \"anchor\" id = \"4.3\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "in this section, instead of estimate only a next word given history of words , we will estimate more than next word given a history. The next words is the all the tokens in the training set, in other words, we will compute all the probabilities of being the next word one of the tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "      #estimate_probability --> used to Estimate the probability of a next word using \n",
    "                                the n-gram counts with k-smoothing \n",
    "                                          \n",
    "    Arguments:\n",
    "      #previous_n_gram -->  a sequence of n words\n",
    "      #n_gram_counts --> the number of times the history of the words appears together \n",
    "      #n_plus1_gram_counts --> the number of times the history of the words and the next word appear together\n",
    "      #vocabulary_size --> the number of words in the vocabulary\n",
    "      #K --> represents the smoothing parameter which is  the value of K for add-k somthing technique\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #probabilities --> a dictionary whose every key is a token from the training set and whose every \n",
    "                         value is the probability for a key (token) to be the next word\n",
    "                         \n",
    "                         \n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Convert the list to tuple to use it as a dictionary key since \n",
    "    #the builtin list type should not be used as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    ##add <e> and <unk> to the vocabulary\n",
    "    #we don't need to add <s> as we compute the chance for every token to be the next word \n",
    "    #so, there's no chance to be the first word\n",
    "    vocabulary = vocabulary + [\"<e>\",\"<unk>\"]\n",
    "    \n",
    "    #compute the vocabulary size \n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    \n",
    "    #Iniatilize the dictionary probabilities which holds the probailities for every token \n",
    "    probabilities = {}\n",
    "    \n",
    "    #Loop for every token(word) in the vocabulary \n",
    "    for word in vocabulary:\n",
    "        \n",
    "        #Compute the probaility of that word to be the next word\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        \n",
    "        #Add this value(probability) to the key(word) in the dictionary (probabilities)\n",
    "        probabilities[word] = probability\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "sentences =  [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"Bi-gram:\\n\",bigram_counts)\n",
    "estimate_probabilities(\"i\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test \n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exrta test \n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize n-gram counts in a matrix form <a anchor = \"anchor\" id = \"4.4\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "        \n",
    "    '''\n",
    "    Usage:\n",
    "      #make_count_matrix --> to create the n-gram counts matrix\n",
    "                                          \n",
    "    Arguments:\n",
    "      #n_plus1_gram_counts --> the number of times the history of the words,\n",
    "                               and the next word appear together\n",
    "                               \n",
    "      #Vocabulary --> list of the unique words in the training set \n",
    "     \n",
    "    \n",
    "    Returns:\n",
    "      #count_matrix --> a matrix of n-gram counts \n",
    "                         \n",
    "      \n",
    "    '''\n",
    "    ##add <e> and <unk> to the vocabulary\n",
    "    #we don't need to add <s> as we compute the chance for every token to be the next word \n",
    "    #so, there's no chance to be the first word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = [] #initialie empty list to hold all the unique n-grams\n",
    "    \n",
    "    #Loop over every n-gram \n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        \n",
    "        #Get all tokens except the last one which represents the next word \n",
    "        #Then, append them in n_grams\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    \n",
    "    #Remove the repeated tokens to hold just the unique ones \n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)} # {key: value for vars in iterable}\n",
    "\n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "\n",
    "    \n",
    "    #Compute the number of columns and the number of rows\n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    \n",
    "    #Pre-allocating  matrix of zeros\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    \n",
    "    #we call apply items() to use the value and key separately\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1] #represents the history of the words\n",
    "        word = n_plus1_gram[-1]     #represents the next word\n",
    "        \n",
    "        #check if the next word in the vocabulary, skip the iteration\n",
    "        #otherwise, get index i,j of the matrix to assign the count \n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram] #Get the index of a specific history \n",
    "        j = col_index[word]   #Get the index of a specific next word\n",
    "        \n",
    "        #Assign count to a specific cell in the matrix\n",
    "        #Count represents the number of times a specific history and a certain next word appear together\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "\n",
    "    #Get the n-gram counts matrix\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "#Show the bi-gram counts matrix \n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the tri-gram counts matrix\n",
    "sentences =  [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "trigram_counts = count_n_grams(sentences,3)\n",
    "\n",
    "#Show the bi-gram counts matrix \n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize n-gram probabilities in a matrix form  <a anchor = \"anchor\" id = \"4.5\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probs_matrix(n_plus1_gram_counts, vocabulary, k = 1):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "      #make_probs_matrix --> to create the n-gram probabilities matrix\n",
    "                                          \n",
    "    Arguments:\n",
    "      #n_plus1_gram_counts --> the number of times the history of the words,\n",
    "                               and the next word appear together\n",
    "                               \n",
    "      #Vocabulary --> list of the unique words in the training set \n",
    "      #k --> the smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "      #probs_matrix --> a matrix of n-gram probabilities \n",
    "                         \n",
    "    '''\n",
    "    \n",
    "    #Compute the n-gram counts matrix\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, vocabulary)\n",
    "    \n",
    "    #increment every count by k to appaly add-k smoothing technique to compute the probabilities\n",
    "    count_matrix += 1 \n",
    "    \n",
    "    #to compute the probabilities , we need the count of every history \n",
    "    #if we sum up every row , we get the count of every history corresponds to every row\n",
    "    #then we divided every row element wise with the corresponding count of a history\n",
    "    probs_matrix = count_matrix.div(count_matrix.sum(axis = 1), axis = 0)\n",
    "    \n",
    "    return probs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "#Show the bi-gram probabilities matrix \n",
    "display(make_probs_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probs_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the tri-gram probabilities matrix \n",
    "sentences =  [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "trigram_counts = count_n_grams(sentences,3)\n",
    "\n",
    "#Show the bi-gram counts matrix \n",
    "display(make_probs_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Perplexity <a anchor = \"anchor\" id = \"5\"></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Perplexity</b> is a metric which tell us how well a probability model predicty a sample.A low perplexity indicates the probability distribution is good at predicting the sample.In our case, we use perplexity to evaluate our n-gram model.\n",
    "\n",
    "<b>Perplexity</b> is defined as: \n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n",
    "\n",
    "- where $N$ is the length of the sentence.\n",
    "- $n$ is the number of words in the n-gram (e.g. 2 for a bigram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    '''\n",
    "    Usage:\n",
    "      #calculate_perplexity --> used to compute perplexity\n",
    "                                          \n",
    "    Arguments:\n",
    "      #sentence: --> list of tokens\n",
    "      #previous_n_gram -->  a sequence of n words\n",
    "      #n_gram_counts --> the number of times the history of the words appears together \n",
    "      #n_plus1_gram_counts --> the number of times the history of the words and the next word appear together\n",
    "      #vocabulary_size --> the number of words in the vocabulary\n",
    "      #K --> represents the smoothing parameter which is  the value of K for add-k somthing technique\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #perplexity -->  a score tells us how well our n-gram model \n",
    "    '''\n",
    "    \n",
    "    #Get the length of the previos words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    \n",
    "    #Prepand <s> and append <e> to the sentence \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    \n",
    "    #Convert the list to tuple to use it as a dictionary key since \n",
    "    #the builtin list type should not be used as a dictionary key\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    #Get the length of the sentence after adding <s> and <e>\n",
    "    N  = len(sentence)\n",
    "    \n",
    "    \n",
    "    #Initialze a variabe to accumulate the product that happens in the denominator \n",
    "    product_pi = 1.0\n",
    "    \n",
    "    #Loop in that range to calculate the denominator of the perplexity\n",
    "    for t in range(n,N):\n",
    "        \n",
    "        #Get the prevoius words (could be just one word or more)\n",
    "        n_gram = sentence[t-n: t]\n",
    "        \n",
    "        #Get the word\n",
    "        word = sentence[t]\n",
    "        \n",
    "        #Compute the language model \n",
    "        prob = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\n",
    "        \n",
    "        #Accumulate the product\n",
    "        product_pi *= 1/prob\n",
    "        \n",
    "        \n",
    "    #Compute the perplexity be getting the N-th root of the product\n",
    "    perplexity = product_pi ** (1/float(N))\n",
    "    \n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "sentences =  [['i', 'love', 'aswan'], ['i', 'love', 'luxor']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"\\nUni-gram:\\n\",unigram_counts)\n",
    "print(\"Bi-gram:\\n\",bigram_counts)\n",
    "\n",
    "#Compute Perplexity \n",
    "PP = calculate_perplexity(sentences[0], unigram_counts, bigram_counts, unique_words , k=1.0)\n",
    "\n",
    "print(\"\\nThe perplexity of this sentence according to our model : \",PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra test\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Build an auto-complete system <a anchor = \"anchor\" id = \"6\" ></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Suggest A Word <a anchor = \"anchor\" id = \"6.1\" > </a>\n",
    "\n",
    "We suggest a word by simply compute probabilities for all possible next words and suggest the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    '''\n",
    "    Usage:\n",
    "      #suggest_a_word --> used to get suggestion for the next word of a previous tokens\n",
    "                                          \n",
    "    Arguments:\n",
    "      #previous_tokens --> list of tokens which we want to predict their next worf\n",
    "      #n_gram_counts --> the number of times the history of the words appears together \n",
    "      #n_plus1_gram_counts --> the number of times the history of the words and the next word appear together\n",
    "      #vocabulary_size --> the number of words in the vocabulary\n",
    "      #K --> represents the smoothing parameter which is  the value of K for add-k somthing technique\n",
    "      #start_with --> if it not None, we assign  a string to it which represents the first few characters of the next word\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #suggestion --> String represents the suggested word\n",
    "      #max_prob --> the probability that appear that word with the previous tokens\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    #Get the length of the previous words corresponds to a certain n-gram (ex: Bi-gram --> n = 1)\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    \n",
    "    #Get the most recent n words to be the pervious words of n-gram sequence\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    #compute probabilities for all possible next words\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    \n",
    "    #Initalize suggestion wich will hold the suggested word\n",
    "    suggestion = None\n",
    "    \n",
    "    #Intialize max_prob which will hold the probability of a specific word \n",
    "    #out of all the possible words that has the highest probability \n",
    "    #to appear with the previous tokens\n",
    "    max_prob = 0\n",
    "    \n",
    "    #List over each tuple in the list of tuples \n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        #If we want the next word to start with a specific charachters\n",
    "        if start_with != None:\n",
    "            \n",
    "            #check if the word don't start with the required charchters\n",
    "            if not word.startswith(start_with):\n",
    "                \n",
    "                #if so , don't consider that word and move to next one\n",
    "                continue\n",
    "                \n",
    "        # Check if this word's probability\n",
    "        # is greater than the current maximum probability\n",
    "        if prob > max_prob:\n",
    "            \n",
    "            #if so, save this word to suggestion for being the best sugggested word so far\n",
    "            suggestion = word\n",
    "            \n",
    "            #also, save their probability \n",
    "            max_prob = prob\n",
    "            \n",
    "            \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the code \n",
    "sentences =  [['i', 'love', 'aswan'], ['i', 'like', 'luxor'],['i','love','Alex']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]  + sentences[2]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "previous_tokens = [\"i\",\"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra test\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Suggest Multiple Words  <a anchor = \"anchor\" id = \"6.2\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    '''\n",
    "    Usage:\n",
    "      #get_suggestion --> used to get multiple suggestion for the next word of a previous tokens\n",
    "                                          \n",
    "    Arguments:\n",
    "      #previous_tokens --> list of tokens which we want to predict their next worf\n",
    "      #n_gram_counts_list --> list of all the different n-grams counts \n",
    "                              Ex: [Uni-gram, Bi-gram,....]\n",
    "      #vocabulary_size --> the number of words in the vocabulary\n",
    "      #K --> represents the smoothing parameter which is  the value of K for add-k somthing technique\n",
    "      #start_with --> if it not None, we assign  a string to it which represents the first few characters of the next word\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "      #suggestions --> list of tuples represents the multiple suggestions\n",
    "    '''\n",
    "    \n",
    "    #Get the length of the given counts of different n-gram \n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    \n",
    "    #Initalize list to hold all the possible suggestions for different n-gram models\n",
    "    suggestions = []\n",
    "    \n",
    "    \n",
    "    #Loop for different n_gram_counts , and n_plus1_gram_counts\n",
    "    for i in range(model_counts - 1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i + 1]\n",
    "        \n",
    "        #Suggest a word based on the given n-gram model \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        \n",
    "        #Append these suggestion to the list , suggestions\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Suggest Multiple Words Using n-grams of Varying Length <a anchor = \"anchor\" id = \"6.3\"></a>\n",
    "In this scetion, we are going to sugggest multiple words with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams...6-grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
