{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of the dataset\n",
    "This Data set is used to know  which of the users purchased/not purchased a particular product\n",
    "\n",
    "# Description\n",
    "<br>\n",
    "<dl>\n",
    "  <dt>User ID</dt>\n",
    "    <dd>It stands for <strong>User Identification</strong>, and it's a bunch of numbers enables system to identify and distinguish between the users who access or use it.</dd>\n",
    "  <dt>Gender</dt>\n",
    "    <dd>The fact of being male or female</dd>\n",
    "  <dt>Age</dt>\n",
    "    <dd>The age of the user</dd>\n",
    "  <dt>Estimated Salary</dt>\n",
    "    <dd>The approximate salary of the user</dd>\n",
    "  <dt>Purchased</dt>\n",
    "    <dd>It's a logical value to know if the user  purchased/not purchased a particular product</dd>\n",
    "    <dd> <strong>1: </strong> means the user purchased the product</dd>\n",
    "    <dd> <strong>0: </strong> means the user didn't purchase the product</dd>\n",
    "    \n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the important libraries\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "train_df = pd.read_csv('Social_Network_Ads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the shape of the training dataset\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of x_train:  (400, 2)\n",
      "The Shape of y_train:  (400, 1)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into features and labels \n",
    "x_train = train_df[train_df.columns[train_df.columns != 'Purchased'] & train_df.columns[train_df.columns != 'User ID'] &train_df.columns[train_df.columns != 'Gender']]\n",
    "y_train = train_df[train_df.columns[train_df.columns == 'Purchased']]\n",
    "\n",
    "#lets print the shape of the features and labels\n",
    "print(\"The Shape of x_train: \",x_train.shape)\n",
    "print(\"The Shape of y_train: \",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  EstimatedSalary\n",
       "0     19            19000\n",
       "1     35            20000\n",
       "2     26            43000\n",
       "3     27            57000\n",
       "4     19            76000\n",
       "..   ...              ...\n",
       "395   46            41000\n",
       "396   51            23000\n",
       "397   50            20000\n",
       "398   36            33000\n",
       "399   49            36000\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the features after removing some of them\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "We remove <strong> \"User ID\" Feature </strong> because it doesn't affect the output so, we don't need it.\n",
    "<br>\n",
    "\n",
    "Also, we remove <strong> \"Gender\" Feature </strong> because we don't explain One-Hot-Encoding in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Training examples: 400\n"
     ]
    }
   ],
   "source": [
    "#Convert the fearues into numpy array so we can feed them to our model\n",
    "X_without_Xo = x_train.values\n",
    "\n",
    "#Define the number of the training examples\n",
    "m = X_without_Xo.shape[0]\n",
    "\n",
    "print(\"The Number of Training examples: {0}\".format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Some Helper Functions\n",
    "<br>\n",
    "<dl>\n",
    "  <dt>sigmod()</dt>\n",
    "  <dd>For applying sigmoid function  on our design matrix, X, to be our hypothsis for non-linear problems</dd>\n",
    "    <dt>featureNormalize()</dt>\n",
    "  <dd>For feature scalling to make the gradient descent converge much more quickly</dd>\n",
    "  <dt>computeCost()</dt>\n",
    "  <dd>For Computing cross entropy cost function for Logistic Regression model </dd>\n",
    "  <dt>gradientDescent()</dt>\n",
    "  <dd>For Updating the parameters</dd>\n",
    "  <dt>pred()</dt>\n",
    "  <dd>For making predictions</dd>\n",
    "</dl>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### The Formula of Sigmoid Function in Vectorized Form:\n",
    "\\begin{equation}\n",
    " sigmoid(z) = \\frac{1}{1 + e^{-\\theta^{T}X}} = \\frac{1}{1 + e^{-X\\theta}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid(X,theta):\n",
    "        '''\n",
    "        Usage:\n",
    "          #sigmoid --> Computes sigmoid of z = X𝜃 element-wise.\n",
    "  \n",
    "        Arguments:\n",
    "          #X --> The Design Matrix\n",
    "          #theta --> The Parameters which need to update\n",
    "    \n",
    "        Returns:\n",
    "          #returns ---> The Value of sigmoid of z\n",
    "        '''\n",
    "        #Compute  Our linear-hypothesis function \n",
    "        z = np.matmul(X,theta)\n",
    "        \n",
    "        #computes the sigmoid of z\n",
    "        g = np.divide(1, (np.add(1, np.exp(-z))))\n",
    "        \n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    '''\n",
    "    Usage:\n",
    "      #featureNormalize--> used for normalizing features\n",
    "  \n",
    "    Arguments:\n",
    "      #X --> The Design Matrix\n",
    "    \n",
    "    Returns:\n",
    "      #The Normalized Matrix\n",
    "      \n",
    "    Notes:\n",
    "      #X is a matrix where each column is a feature and each row is an example\n",
    "      #So, you need to perform the normalization separately for each feature\n",
    "    '''\n",
    "    \n",
    "    #Preallocating some variables to be used later \n",
    "    X_norm = np.copy(X)\n",
    "    mu = np.zeros((1, X.shape[1]))\n",
    "    sigma = np.zeros((1,X.shape[1]))\n",
    "\n",
    "    #compute the mean of the feature and subtract it from the dataset, storing the mean value in mu\n",
    "    #Next, compute the standard deviation of each feature, storing the standard deviation in sigma.\n",
    "    for i in range(X.shape[1]):\n",
    "        mu[0, i] = mu[0, i] + np.mean(X_norm[:, i])\n",
    "        sigma[0, i] = sigma[0, i] + np.std(X_norm[:, i])\n",
    "        \n",
    "    #Finally, compute the standard deviation of each feature and divide each feature by it's standard deviation, storing the result in x_norm\n",
    "    for i in range(X.shape[1]):\n",
    "        X_norm[:, i] = np.divide(np.subtract(X_norm[:, i], mu[0, i]), sigma[0, i])\n",
    "        \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Formula of  Regularized Cross Entropy Cost Functin:\n",
    "\n",
    "\\begin{equation}\n",
    "  ٌRCE = \\sum_{i=1}^{m} {Loss(y_{pred},y)} =\\frac{1}{m}\\sum_{i=1}^{m} {(y^{(i)})(-\\log(y_{pred}^{(i)})) - (l-y^{(i)})(\\log(1-y_{pred}^{(i)}))} \\hspace{0.1cm} +\\hspace{0.1cm} \\frac{\\lambda}{2} \\sum_{j=1}^{n} {\\theta_{j}\\hspace{0.01cm}^2}\n",
    "\\end{equation}\n",
    "\n",
    "### Note: \n",
    "The type of product in this formula is <strong> element-wise multiplication</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta,lambd):\n",
    "    '''\n",
    "    Usage:\n",
    "      #computCost --> computes the cost for logistic regression\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #X --> The Design Matrix\n",
    "      #y --> The Ground Truth\n",
    "      #theta --> The Parameters which need to update\n",
    "      #lambd --> The lambda parameter controls the amount of regularization applied to the model.\n",
    "                The larger lambda is, the more the coefficients are shrunk toward zero.\n",
    "    \n",
    "    Returns:\n",
    "      #The regularized cost value\n",
    "    '''\n",
    "    \n",
    "    #Compute  Our non-linear hypothesis function \n",
    "    h = sigmoid(X,theta)\n",
    "    \n",
    "    #Compute the losses\n",
    "    losses = np.subtract(np.multiply(-y,np.log(h)), np.multiply((1-y),np.log(1-h)))\n",
    "    \n",
    "    #Compute the l2 reularization term \n",
    "    reg_term = (lambd/(2*m))*np.sum(np.power(theta,2))\n",
    "     \n",
    "    #Compute the Cross Entropy Cost function\n",
    "    J = (1/m)*(np.sum(losses)) + reg_term\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### The Formula of the Regularized gradient is:\n",
    "\\begin{cases}\n",
    "\\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{1}{m} \\bigg(\\big(\\sum_{i=1}^{m} { (y_{pred}^{(i)} - y^{(i)}) x^{(i)}_{j} \\big) + \\lambda \\theta_{j}\\bigg)} & \\text{for } \\theta \t\\neq \\theta_{0}\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_{0}} = \\frac{1}{m} \\big(\\sum_{i=1}^{m} { (y_{pred}^{(i)} - y^{(i)}) x^{(i)}_{0} \\big)}& \\text{for } \\theta =  \\theta_{0}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "\n",
    "The type of product in this formula is <strong> element-wise multiplication</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,lambd,num_iters):\n",
    "    '''\n",
    "    Usage:\n",
    "      #gradientDescent --> computes the gradient descent for linear regression\n",
    "  \n",
    "    \n",
    "    Arguments:\n",
    "      #X --> The Design Matrix\n",
    "      #y --> The Ground Truth\n",
    "      #theta --> The Parameters which need to update\n",
    "      #alpha --> is the learning rate which indicates the learning step or how far we go down \n",
    "      #lambd --> The lambda parameter controls the amount of regularization applied to the model.\n",
    "                 The larger lambda is, the more the coefficients are shrunk toward zero.\n",
    "      #num_iters--> is the number of iterations needed to go to the global optima\n",
    "    \n",
    "    Returns:\n",
    "      #The updated parameters,theta \n",
    "      #cost_history: which is list containing the the values of the cost function, J, for every iteration\n",
    "    '''\n",
    "    #Define the cost history as empty list\n",
    "    cost_history = []\n",
    "    \n",
    "    #Preallocating gradient for faster computaions \n",
    "    #The size of gradient equals:(numfeatures (includingx_0),)\n",
    "    dtheta = np.zeros((X.shape[1],))\n",
    "    dtheta_reg = np.zeros((X.shape[1],))\n",
    "    \n",
    "    #Compute sigmoid of X𝜃 element-wise with the parameters, theta, that we intialize\n",
    "    h = sigmoid(X,theta)\n",
    "    \n",
    "    theta_zero = np.array([theta[0]])\n",
    "    \n",
    "    theta_rest = theta[1:]\n",
    "    \n",
    "    #Keep until Convergence\n",
    "    for i in range(num_iters+1):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #dtheta_reg is the partial derivates of cost function with respect to the parameters, theta, for regularized parameters\n",
    "        dtheta_reg = (1/m)*((np.matmul((X[:,1:].T), (np.subtract(h, y)))) + (lambd * theta[1:]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #dtheta is the partial derivates of cost function with respect to the parameters, theta, for unregularized  parameter theta[0]\n",
    "        #Note, dtheta is 3-vector for dtheta[0] to dtheta[2] but we just need dtheta[0] to update theta[0]\n",
    "        #So, we will slicing dtheta during updating the un regularized parametet theta[0] to just update it\n",
    "        #And for theta[1] and theta[2], we will use dtheta_reg\n",
    "        dtheta = (1/m)*(np.matmul(X[:,0], (np.subtract(h, y))))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Update theta[0]\n",
    "        theta_zero = theta_zero - dtheta\n",
    "        #Update the rest of the parameters, theta[1] and theta[2]\n",
    "        theta_rest = theta_rest - dtheta_reg \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Concatenate theta_zero with the rest of the parameters and assign them to theta\n",
    "        theta = np.concatenate((theta_zero, theta_rest), axis = 0)\n",
    "        \n",
    "        \n",
    "\n",
    "        #While debugging, it can be useful to print out the values of the cost function (computeCost) \n",
    "        #Note we compute the regulaized cost and feed it parameters theta even theta[0]\n",
    "        #That's not true because theta[0]  belongs to unregulrized cost function \n",
    "        #So, you may find fluctuation in its value\n",
    "        cost = computeCost(X,y,theta,lambd)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Append the value of the cost at a specific value for theta to cost_history\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        #print the cost function for every itration to track its new value step-by-step\n",
    "        print(\"Reached iteration: {0}, the cost = {1}\".format(i, cost))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nParameters have been trained!\") \n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(input_pred,theta):\n",
    "    '''\n",
    "    Usage:\n",
    "      #pred --> used to predict the output of the input\n",
    "      \n",
    "    Arguments:\n",
    "      #input_pred --> the input you want to predict its output \n",
    "      #theta --> the updated param\n",
    "    \n",
    "    Returns:\n",
    "      #The predicted output\n",
    "    '''\n",
    "    #First, Normalize the input \n",
    "    #input_pred,_,_ = featureNormalize(input_pred)\n",
    "    \n",
    "    #Compute the prediction\n",
    "    prediction = sigmoid(input_pred,theta)\n",
    "    \n",
    "    print(\"The output: {0}\\n\".format(prediction))\n",
    "    \n",
    "    #check if the prediction is greater than 0.5 , the user purchased the product \n",
    "    if (np.greater(prediction,0.5) == True):\n",
    "        print(\"Result: The user purchased the product \")\n",
    "        \n",
    "    #if not, the user didn't purchase the product\n",
    "    else:\n",
    "        print(\"Result: The user didn't purchase the product \")\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we normalize the features\n",
    "#_,_ indicate that we don't want to return mu and sigma so the function just return the normalize features\n",
    "X_normalized,_,_ = featureNormalize(X_without_Xo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then We concatenate x_0 with X_normalized \n",
    "\n",
    "#Create array of ones , which representes the x_0, to combine it with the rest of the features\n",
    "ones = np.ones((400,1))\n",
    "\n",
    "#Combining, so the shape of  the features will be (400,5) \n",
    "X = np.concatenate((ones, X_normalized), axis = 1)\n",
    "\n",
    "#(Optional) --> lets reduce the rank of the output, y, to be (400,) instead of (400,1) \n",
    "#And, convert it to numpy array  of size (400,)\n",
    "y = y_train.values.reshape(400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of x:  (400, 3)\n",
      "The Shape of y:  (400,)\n"
     ]
    }
   ],
   "source": [
    "#Print the shape of the final features matrix  and the labels\n",
    "print(\"The Shape of x: \",X.shape)\n",
    "print(\"The Shape of y: \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached iteration: 0, the cost = 0.6390567861871883\n",
      "Reached iteration: 1, the cost = 0.5950263749030088\n",
      "Reached iteration: 2, the cost = 0.5604242495850167\n",
      "Reached iteration: 3, the cost = 0.5343450828549332\n",
      "Reached iteration: 4, the cost = 0.5157602860348304\n",
      "Reached iteration: 5, the cost = 0.5036408636460823\n",
      "Reached iteration: 6, the cost = 0.49703443829334476\n",
      "Reached iteration: 7, the cost = 0.495101145118544\n",
      "\n",
      "\n",
      "Parameters have been trained!\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "theta, cost_history = gradientDescent(X,y,theta=np.array([0,0,0]),alpha = 0.0000000000001,lambd= 0.00000000003,num_iters = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.14,  1.33,  0.85])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ground Truth: 0\n",
      "The output: 0.03489140853175609\n",
      "\n",
      "Result: The user didn't purchase the product \n"
     ]
    }
   ],
   "source": [
    "#Lets make prediction, in this case we wil make predictions one of the training examples \n",
    "print(\"The Ground Truth: {0}\".format(y[0]))\n",
    "prediction = pred(X[0], theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
